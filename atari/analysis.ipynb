{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import yaml\n",
    "from matplotlib import pyplot as plt\n",
    "plt.switch_backend(\"agg\")\n",
    "import numpy as np\n",
    "import torch\n",
    "import cv2\n",
    "from rl import RL\n",
    "from atari import AtariEnv\n",
    "from experiment import DQNExperiment\n",
    "\n",
    "\n",
    "root_dir = r\"./results/pong0\"\n",
    "params = yaml.safe_load(open(os.path.join(root_dir, \"config.yaml\"), 'r'))\n",
    "\n",
    "# outputs\n",
    "if not os.path.exists(os.path.join(root_dir, 'figs')):\n",
    "    os.mkdir(os.path.join(root_dir, 'figs'))\n",
    "    for item in ['game', 'mu', 'gqtl', 'mugq', 'obs']:\n",
    "        f = os.path.join(root_dir, 'figs', item)\n",
    "        if not os.path.exists(f):\n",
    "            os.mkdir(f)\n",
    "\n",
    "np.random.seed(params['random_seed'])\n",
    "torch.manual_seed(params['random_seed'])\n",
    "random_state = np.random.RandomState(params['random_seed'])\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "env = AtariEnv(game_name=params['game_name'], rendering=False, sticky_actions=False, frame_skip=params['frame_skip'], \n",
    "               terminal_on_life_loss=params['terminal_on_life_loss'], screen_size=params['screen_size'])\n",
    "\n",
    "network_size = params['network_size']\n",
    "ai = RL(state_shape=env.state_shape, nb_actions=env.nb_actions, action_dim=params['action_dim'],\n",
    "        reward_dim=params['reward_dim'], history_len=params['history_len'], gamma=params['gamma'], \n",
    "        learning_rate=params['learning_rate'], epsilon=params['epsilon'], final_epsilon=params['final_epsilon'],\n",
    "        test_epsilon=params['test_epsilon'], annealing_steps=params['annealing_steps'], minibatch_size=params['minibatch_size'],\n",
    "        replay_max_size=params['replay_max_size'], update_freq=params['update_freq'], \n",
    "        learning_frequency=params['learning_frequency'], ddqn=params['ddqn'], network_size=network_size, \n",
    "        normalize=params['normalize'], event=params['event'], sided_Q=params['sided_Q'], rng=random_state, device=\"cpu\")\n",
    "\n",
    "network_weights_file = os.path.join(root_dir, 'ai/q_network_weights_201.pt')\n",
    "ai.load_weights(weights_file_path=network_weights_file)\n",
    "\n",
    "expt = DQNExperiment(env=env, ai=ai, episode_max_len=params['episode_max_len'], annealing=params['annealing'],\n",
    "                        history_len=params['history_len'], max_start_nullops=params['max_start_nullops'],\n",
    "                        replay_min_size=params['replay_min_size'], test_epsilon=params['test_epsilon'], \n",
    "                        folder_location=params['folder_location'], folder_name=params['folder_name'], score_window_size=100, \n",
    "                        make_folder=False, rng=random_state)\n",
    "\n",
    "\n",
    "def tensor_linspace(start: torch.Tensor, stop: torch.Tensor, num: int):\n",
    "    steps = torch.arange(num, dtype=torch.float32, device=start.device) / (num - 1)\n",
    "    for _ in range(start.ndim):\n",
    "        steps = steps.unsqueeze(-1)\n",
    "\n",
    "    out = start[None] + steps*(stop - start)[None]\n",
    "    return out\n",
    "\n",
    "\n",
    "def g_computer(x, x2, model, multiplier, compute_steps, filter_th):\n",
    "    \"\"\"\n",
    "    outputs: \n",
    "    `result`: component-wise g-formula \n",
    "    `measure`: sum(results) == expt. change of grit/reachability computed from g-formula\n",
    "    `ttl_grads`: component-wise integral of grads from x to x2\n",
    "    \"\"\"\n",
    "    x = torch.Tensor(x)\n",
    "    x2 = torch.Tensor(x2)\n",
    "    states = tensor_linspace(x, x2, compute_steps)    # -> make it numpy\n",
    "    result = torch.zeros_like(x)\n",
    "    ttl_grads = torch.zeros_like(x)\n",
    "    measure = 0\n",
    "    for i in range(compute_steps-1):\n",
    "        s = states[i].detach()\n",
    "        s2 = states[i+1].detach()\n",
    "        max_a = model.get_max_action(s)\n",
    "        max_a2 = model.get_max_action(s2)\n",
    "        grad = multiplier * model.get_grad(s)[max_a].squeeze()\n",
    "        grad2 = multiplier * model.get_grad(s2)[max_a2].squeeze()\n",
    "        grad[abs(grad) < filter_th] = 0  # denoising\n",
    "        grad2[abs(grad2) < filter_th] = 0 \n",
    "        mu = (s2 - s) / model.normalize  # TODO get rid of all the ai.normalize (should instead be done at env)\n",
    "        grads = 0.5 * (grad + grad2)\n",
    "        y = torch.mul(mu, grads.squeeze(dim=0))  # Note: dt is cancelled: denominator of mu and multiplication in the integral\n",
    "        result += y.detach()\n",
    "        measure += torch.sum(y.detach())\n",
    "        ttl_grads += grads.detach()\n",
    "    return result, measure, ttl_grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAD_FILTER_TH = 0.1\n",
    "\n",
    "d_grit = []\n",
    "d_grit_decomp = []\n",
    "grits = []\n",
    "max_actions = []\n",
    "\n",
    "print(\"Is grit: \", expt.ai.sided_Q)\n",
    "multiplier = -1 if expt.ai.sided_Q == 'grit' else 1\n",
    "\n",
    "expt.env.reset()\n",
    "expt._episode_reset()\n",
    "game_over = False\n",
    "max_steps = 70\n",
    "for step in range(max_steps):\n",
    "    if game_over:\n",
    "        print(\"Game-over before max steps.\")\n",
    "        break\n",
    "\n",
    "    plt.close(\"all\")\n",
    "    expt.last_episode_steps += 1\n",
    "    \n",
    "    if step <= 60:  # some no-ops\n",
    "        if step % 10 == 0:\n",
    "            print(\"step:\", step)\n",
    "        action = 0\n",
    "        grit = multiplier * expt.ai.get_q(expt.last_state)[0]\n",
    "        max_a = expt.ai.get_max_action(expt.last_state)\n",
    "    else:\n",
    "        grit = multiplier * expt.ai.get_q(expt.last_state)[0]\n",
    "        max_a = expt.ai.get_max_action(expt.last_state)\n",
    "        ###### Uncomment the following lines to manually control the game\n",
    "        # print('Q >> ', grit)\n",
    "        # print('Greedy action: ', max_a)\n",
    "        # print('--')\n",
    "        # action = input('action >> ')\n",
    "        # # action = 0\n",
    "        # # action = max_a\n",
    "        # action = int(action)\n",
    "        # if action >= expt.env.nb_actions:\n",
    "        #     print('Unknown action.')\n",
    "        #     continue\n",
    "    \n",
    "    grits.append(grit)\n",
    "    max_actions.append(max_a)\n",
    "    grit = grit[max_a]\n",
    "\n",
    "    new_obs, reward, game_over, _ = expt.env.step(action)\n",
    "    expt.env.save_img(os.path.join(root_dir, 'figs', 'game', 'pong'+str(expt.last_episode_steps)+\".png\"))\n",
    "\n",
    "    prev_state = expt.last_state.copy()\n",
    "    expt._update_state(new_obs)  # --> expt.last_state is now x'\n",
    "\n",
    "    mu = (expt.last_state - prev_state) / expt.ai.normalize  # x_k - x_{k-1} --> for plotting\n",
    "    \n",
    "    g, measure, grad_total = g_computer(x=prev_state, x2=expt.last_state, model=expt.ai, multiplier=multiplier,\n",
    "                                        compute_steps=10, filter_th=GRAD_FILTER_TH)\n",
    "    \n",
    "    torch.clamp_(g, min=0, max=1)  # only the causal ones (ignoring negative g) for plotting\n",
    "    d_grit_decomp.append(measure)     # change of grit computed via Decomp Lemma\n",
    "    grit2 = multiplier * np.max(expt.ai.get_q(expt.last_state)[0])\n",
    "    d_grit.append(grit2 - grit)       # actual change of grit\n",
    "\n",
    "    ## PLOTS:\n",
    "\n",
    "    ## obs:\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(1.5, 1.5), dpi=300)\n",
    "    ax.imshow(new_obs, cmap=\"gray\")\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    _ = ax.set_xticklabels([])\n",
    "    _ = ax.set_yticklabels([])\n",
    "    fig.tight_layout(pad=0, h_pad=0, w_pad=0)\n",
    "    fig.savefig(os.path.join(root_dir, 'figs', 'obs', 'obs__'+str(expt.last_episode_steps)+\".png\"))\n",
    "\n",
    "    ## mu:\n",
    "    x = np.flip(mu[3, :, :], axis=0)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(1.5, 1.5), dpi=300)\n",
    "    plt.pcolor(x, vmin=-1, vmax=1, cmap=\"seismic\")\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    _ = ax.set_xticklabels([])\n",
    "    _ = ax.set_yticklabels([])\n",
    "    fig.tight_layout(pad=0, h_pad=0, w_pad=0)\n",
    "    fig.savefig(os.path.join(root_dir, 'figs', 'mu', 'mu__'+str(expt.last_episode_steps)+\".png\"))\n",
    "\n",
    "    ## total grad:\n",
    "    x = np.flip(grad_total[3, :, :].numpy(), axis=0)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(1.5, 1.5), dpi=300)\n",
    "    plt.pcolor(x, vmin=-1, vmax=1, cmap=\"seismic\")\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    _ = ax.set_xticklabels([])\n",
    "    _ = ax.set_yticklabels([])\n",
    "    fig.tight_layout(pad=0, h_pad=0, w_pad=0)\n",
    "    fig.savefig(os.path.join(root_dir, 'figs', 'gqtl', 'gqtl'+str(expt.last_episode_steps)+\".png\"))\n",
    "\n",
    "    ## g:\n",
    "    x = np.flip(g[3, :, :].numpy(), axis=0)\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(1.5, 1.5), dpi=300)\n",
    "    plt.pcolor(x, vmin=-0.05, vmax=0.05, cmap=\"seismic\")\n",
    "    ax.xaxis.set_ticks_position('none')\n",
    "    ax.yaxis.set_ticks_position('none')\n",
    "    _ = ax.set_xticklabels([])\n",
    "    _ = ax.set_yticklabels([])\n",
    "    fig.tight_layout(pad=0, h_pad=0, w_pad=0)\n",
    "    fig.savefig(os.path.join(root_dir, 'figs', 'mugq', 'mugq'+str(expt.last_episode_steps)+\".png\"))\n",
    "\n",
    "    plt.close(\"all\")\n",
    "    if not game_over and expt.last_episode_steps >= expt.episode_max_len:\n",
    "        print('Reaching maximum number of steps in the current episode.')\n",
    "        game_over = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atoi = lambda text: int(text) if text.isdigit() else text\n",
    "natural_keys = lambda text: [ atoi(c) for c in re.split(r'(\\d+)', text) ]\n",
    "\n",
    "def make(image_folder):\n",
    "    subfolders = ['game', 'mu', 'gqtl', 'mugq', 'obs']\n",
    "    images = dict()\n",
    "    for sf in subfolders:\n",
    "        f = os.path.join(image_folder, sf)\n",
    "        images[sf] = [img for img in os.listdir(f) if img.endswith(\".jpg\") or img.endswith(\"png\")]\n",
    "        images[sf].sort(key=natural_keys)\n",
    "        images[sf] = [cv2.imread(os.path.join(f, item)) for item in images[sf]]\n",
    "    return images\n",
    "\n",
    "\n",
    "fig_dir = os.path.join(root_dir, 'figs')\n",
    "frames = make(fig_dir)\n",
    "min_grit = [np.min(k) for k in grits]\n",
    "num_frames = 11  # 8\n",
    "start = 46  # 41, 49\n",
    "\n",
    "fig, axs = plt.subplots(5, num_frames, figsize=(9, 4), dpi=300)  # (6.5, 4)\n",
    "\n",
    "for i in range(num_frames):\n",
    "    # axs[0, i].imshow(np.flip(frames['game'][start + i], axis=2))\n",
    "    axs[0, i].imshow(frames['obs'][start + i])\n",
    "    axs[1, i].imshow(np.flip(frames['mu'][start + i], axis=2))\n",
    "    axs[2, i].imshow(np.flip(frames['gqtl'][start + i], axis=2))\n",
    "    axs[3, i].imshow(np.flip(frames['mugq'][start + i], axis=2))\n",
    "    axs[4, i].axhline(y=0, color=\"lightgrey\", ls=\"-\", lw=0.5)\n",
    "    axs[4, i].axhline(y=0.25, color=\"lightgrey\", ls=\"-\", lw=0.5)\n",
    "    axs[4, i].axhline(y=0.5, color=\"lightgrey\", ls=\"-\", lw=0.5)\n",
    "    axs[4, i].axhline(y=0.75, color=\"lightgrey\", ls=\"-\", lw=0.5)\n",
    "    axs[4, i].axhline(y=1, color=\"lightgrey\", ls=\"-\", lw=0.5)\n",
    "    axs[4, i].plot(min_grit[start + i], \"ko\", ms=3)\n",
    "    axs[4, i].bar(0, min_grit[start + i], width=0.2, color=\"orange\")\n",
    "    for ax in axs[:, i]:\n",
    "        ax.xaxis.set_ticks_position('none')\n",
    "        ax.yaxis.set_ticks_position('none')\n",
    "        _ = ax.set_xticklabels([])\n",
    "        _ = ax.set_yticklabels([])\n",
    "        ax.margins(0)\n",
    "    # axs[4, i].set_frame_on(False)\n",
    "    axs[4, i].set_xlim(-0.5,0.5)\n",
    "    axs[4, i].set_ylim(-0.02,1.02)\n",
    "# fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)\n",
    "fig.tight_layout(pad=0.08, h_pad=0.2, w_pad=0.2)\n",
    "fig.savefig(os.path.join(fig_dir, \"full_\" + str(start) + \"_\" + str(start+num_frames-1) + \".pdf\"))\n",
    "plt.close(\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_grit = [np.min(k) for k in grits]\n",
    "\n",
    "num_frames = len(frames[\"game\"])\n",
    "start = 0\n",
    "\n",
    "if not os.path.exists(os.path.join(fig_dir, 'video')):\n",
    "    os.mkdir(os.path.join(fig_dir, 'video'))\n",
    "\n",
    "for i in range(num_frames):\n",
    "    fig, axs = plt.subplots(2, 3, figsize=(6, 4), dpi=300)\n",
    "    axs[0, 0].imshow(np.flip(frames['game'][start + i], axis=2))\n",
    "    axs[0, 1].imshow(frames['obs'][start + i])\n",
    "    axs[0, 2].imshow(np.flip(frames['mu'][start + i], axis=2))\n",
    "    axs[1, 0].imshow(np.flip(frames['gqtl'][start + i], axis=2))\n",
    "    axs[1, 1].imshow(np.flip(frames['mugq'][start + i], axis=2))\n",
    "    axs[1, 2].axhline(y=0, color=\"lightgrey\", ls=\"-\", lw=0.5)\n",
    "    axs[1, 2].axhline(y=0.25, color=\"lightgrey\", ls=\"-\", lw=0.5)\n",
    "    axs[1, 2].axhline(y=0.5, color=\"lightgrey\", ls=\"-\", lw=0.5)\n",
    "    axs[1, 2].axhline(y=0.75, color=\"lightgrey\", ls=\"-\", lw=0.5)\n",
    "    axs[1, 2].axhline(y=1, color=\"lightgrey\", ls=\"-\", lw=0.5)\n",
    "    axs[1, 2].plot(min_grit[start + i], \"ko\", ms=3)\n",
    "    axs[1, 2].bar(0, min_grit[start + i], width=0.2, color=\"orange\")\n",
    "    for ax in axs.ravel():\n",
    "        ax.xaxis.set_ticks_position('none')\n",
    "        ax.yaxis.set_ticks_position('none')\n",
    "        _ = ax.set_xticklabels([])\n",
    "        _ = ax.set_yticklabels([])\n",
    "        ax.margins(0)\n",
    "    axs[0, 0].set_frame_on(False)\n",
    "    axs[1, 2].set_xlim(-0.5,0.5)\n",
    "    axs[1, 2].set_ylim(-0.02,1.02)\n",
    "    # fig.subplots_adjust(left=0, bottom=0, right=1, top=1, wspace=0, hspace=0)\n",
    "    fig.tight_layout(pad=0.3, h_pad=0.5, w_pad=0.5)\n",
    "    fig.savefig(os.path.join(fig_dir, 'video', \"v_\" + str(i) + \".png\"))\n",
    "    plt.close(\"all\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "9008939de075258e4d84b8d053caf771a9d331e30296380fbec5834e342cbc60"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
